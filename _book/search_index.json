[["index.html", "Least Squares Euclidean Multidimensional Scaling Note", " Least Squares Euclidean Multidimensional Scaling Jan de Leeuw Started October 02, 2020. Last update January 30, 2024 Note This book will be expanded/updated frequently. The directory github.com/deleeuw/stress has a pdf version, a html version, the bib file, the complete Rmd file with the codechunks, and the R and C source code. All suggestions for improvement of text or code are welcome, and some would be really beneficial. For example, I only use base R graphics, nothing more fancy, because base graphics is all I know. All text and code are in the public domain and can be copied, modified, and used by anybody in any way they see fit. Attribution will be appreciated, but is not required. For completeness we include a slighty modified version of the Unlicense as appendix ??. I number and label all displayed equations. Equations are displayed, instead of inlined, if and only if one of the following is true. They are important. They are referred to elsewhere in the text. Not displaying them messes up the line spacing. All code chunks in the text are named. Theorems, lemmas, chapters, sections, subsections and so on are also named and numbered, using bookdown/Rmarkdown. I have been somewhat hesitant to use lemmas, theorems, and corollaries in this book. But ultimately they enforce precision and provide an excellent organizational tool. If there is a proof of a lemma, theorem, or corollary, it ends with a \\(\\square\\). Another idiosyncracy: if a line in multiline displayed equation ends with “=”, then the next line begins with “=”. If it ends with “+”, then the next line begin with “+”, and if it ends with “-” the next line begins with “+” as well. I’ll try to avoid ending a line with “+” or “-”, especially with “-”, but if it happens you are warned. A silly example is \\[\\begin{align} &amp;(x+y)^2-\\\\ &amp;+4x=\\\\ &amp;=x^2+y^2-2x=\\\\ &amp;=(x-y)^2\\geq\\\\ &amp;\\geq 0. \\end{align}\\] Just as an aside: if I refer to something that has been mentioned “above” I mean something that comes earlier in the book and “below” refers to anything that comes later. This always confuses me, so I had to write it down. The dilemma of whether to use “we” or “I” throughout the book is solved in the usual way. If I feel that a result is the work of a group (me, my co-workers, and the giants on whose shoulders we stand) then I use “we”. If it’s an individual decision, or something personal, then I use “I”. The default is “we”, as it always should be in scientific writing. Most of the individual chapters also have some of the necessary mathematical background material, both notation and results, sometimes with specific eleborations that seem useful for the book. Sometimes this background material is quite extensive. Examples are splines, majorization, unweighting, monotone regression, and the basic Zangwill and Ostrowski fixed point theorems we need for convergence analysis of our algorithms. There is an appendix ?? with code, and an appendix ?? with data sets. These contain brief descriptions and links to the supplementary materials directories, which contain the actual code and data. Something about code and R/C I will use this note to thank Rstudio, in particular J.J. Allaire and Yihui Xi, for their contributions to the R universe, and for their promotion of open source software and open access publications. Not too long ago I was an ardent LaTeX user, firmly convinced I would never use anything else again in my lifetime. In the same way thatI was convinced before I would never use anything besides, in that order, FORTRAN, PL/I, APL, and (X)Lisp. And PHP/Apache/MySQL. But I lived too long. And then, in my dotage, lo and behold, R, Rstudio, (R)Markdown, bookdown, blogdown, Git, Github, Netlify came along. Figure 0.1: Forrest Young, Bepi Pinner, Jean-Marie Bouroche, Yoshio Takane, Jan de Leeuw at La Jolla, August 1975 "],["preface.html", "Preface", " Preface This book is definitely not an impartial and balanced review of all of multidimensional scaling (MDS) theory and history. It emphasizes computation, and the mathematics needed for computation. In addition, it is a summary of over 50 years of MDS work by me, either solo or together with my many excellent current or former co-workers and co-authors. It is heavily biased in favor of the smacof formulation of MDS (De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Mair (2009)), and the corresponding majorization (or MM) algorithms. And, moreover, I am shamelessly squeezing in as many references to my published and unpublished work as possible, with links to the corresponding pdf’s if they are available. Thus this book is also a jumpstation into my bibliography. I have not organized the book along historical lines because most of the early techniques and results have been either drastically improved or completely abandoned. Nevertheless, some personal historical perspective may be useful. I will put most of it in this preface, so uninterested readers can easily skip it. I got involved in MDS in 1968 when John van de Geer returned from a visit to Clyde Coombs in Michigan and started the Department of Data Theory in the Division of Social Sciences at Leiden University. I was John’s first hire, although I was still a graduate student at the time. Remember that Clyde Coombs was running the Michigan Mathematical Psychology Program, and he had just published his remarkable book “A Theory of Data” (Coombs (1964)). The name of the new department in Leiden was taken from the title of that book, and Coombs was one of the first visitors to give a guest lecture there. This is maybe the place to clear up some possible misunderstandings about the name “Data Theory”. Coombs was mainly interested in a taxonomy of data types, and in pointing out that “data” were not limited to a table or data-frame of objects by variables. In addition, there were also similarity ratings, paired comparisons, and unfolding data. Coombs also emphasized that data were often non-metric, i.e. ordinal or categorical, and that it was possible to analyze these ordinal or categorical relationships directly, without first constructing numerical scales to which classical techniques could be applied. One of the new techniques discussed in Coombs (1964) was a ordinal form of MDS, in which not only the data but also the representation of the data in Euclidean space were non-metric. John van de Geer had just published Van de Geer (1967). In that book, and in the subsequent book Van de Geer (1971), he developed his unique geometric approach to multivariate analysis. Relationship between variables, and between variables and individuals, were not just discussed using matrix algebra, but were also visualized in diagrams. This was related to the geometric representations in Coombs’ Theory of Data, but it concentrated on numerical data in the form of rectangular matrices of objects by variables. Looking back it is easy to see that both Van de Geer and Coombs influenced my approach to data analysis. I inherited the emphasis on non-metric data and on visualization. But, from the beginning, I interpreted “Data Theory” as “Data Analysis”, with my emphasis shifting to techniques, loss functions, implementations, algorithms, optimization, computing, and programming. This is of interest because in 2020 my former Department of Statistics at UCLA, together with the Department of Mathematics, started a bachelor’s program in Data Theory, in which “Emphasis is placed on the development and theoretical support of a statistical model or algorithmic approach. Alternatively, students may undertake research on the foundations of data science, studying advanced topics and writing a senior thesis.” This sounds like a nice hybrid of Data Theory and Data Analysis, with a dash of computer science mixed in. Computing and optimization were in the air in 1968, not so much because of Coombs, but mainly because of Roger Shepard, Joe Kruskal, and Doug Carroll at Bell Labs in Murray Hill. John’s other student Eddie Roskam and I were fascinated by getting numerical representations from ordinal data by minimizing explicit least squares loss functions. Eddie wrote his dissertation in 1968 (Roskam (1968)). In 1973 I went to Bell Labs for a year, and Eddie went to Michigan around the same time to work with Jim Lingoes, resulting in Lingoes and Roskam (1973). My first semi-publication was De Leeuw (1968), quickly followed by a long sequence of other, admittedly rambling, internal reports. Despite this very informal form of publication the sheer volume of them got the attention of Joe Kruskal and Doug Carroll, and I was invited to spend the academic year 1973-1974 at Bell Laboratories. That visit somewhat modified my cavalier approach to publication, but I did not become half-serious in that respect until meeting with Forrest Young and Yoshio Takane at the August 1975 US-Japan seminar on MDS in La Jolla. Together we used the alternating least squares approach to algorithm construction that I had developed since 1968 into a quite formidable five-year publication machine, with at its zenith Takane, Young, and De Leeuw (1977). In La Jolla I gave the first presentation of the majorization method for MDS, later known as smacof, with the first formal convergence proof. The canonical account of smacof was published in a conference paper (De Leeuw (1977)). Again I did not bother to get the results into a journal or into some other more effective form of publication. The basic theory for what became known as smacof was also presented around the same time in another book chapter De Leeuw and Heiser (1977). In 1978 I was invited to the Fifth International Symposium on Multivariate Analysis in Pittsburgh to present what became De Leeuw and Heiser (1980). There I met Nan Laird, one of the authors of the basic paper on the EM algorithm (Dempster, Laird, and Rubin (1977)). I remember enthusiastically telling her on the conference bus that EM and smacof were both special case of the general majorization approach to algorithm construction, which was consequently born around the same time. But that is a story for a companion volume, which currently only exists in a very preliminary stage (https://github.com/deleeuw/bras). My 1973 PhD thesis (De Leeuw (1973), reprinted as De Leeuw (1984)) was actually my second attempt at a dissertation. I had to get a PhD, any PhD, before going to Bell Labs, because of the difference between the Dutch and American academic title and reward systems. I started writing a dissertation on MDS, in the spirit of what later became De Leeuw and Heiser (1982). But halfway through I lost interest and got impatient, and I decided to switch to nonlinear multivariate analysis. This second attempt did produced a finished dissertation (De Leeuw (1973)), which grew over time, with the help of multitudes, into Gifi (1990). But that again is a different history, which I will tell some other time in yet another companion volume (https://github.com/deleeuw/gifi). For a long time I did not do much work on MDS, until the arrival of Patrick Mair and the R language led to a resurgence of my interest, and ultimately to De Leeuw and Mair (2009) and (mair_groenen_deleeuw_A_19?). I consider this MDS book to be a summary and extension of the basic papers De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Heiser (1980), De Leeuw and Heiser (1982), and De Leeuw (1988) (published version of (deleeuw_R_84c?)), all written 30-40 years ago. Footprints in the sands of time. It can also be seen as an elaboration of the more mathematical and computational sections of the excellent and comprehensive textbook of Borg and Groenen (2005). That book has much more information about the origins, the data, and the applications of MDS, as well as on the interpretation of MDS solutions. In this book I concentrate almost exclusively on the mathematical, computational, and programming aspects of MDS. For those who cannot get enough of me, there is a data base of my published and unpublished reports and papers since 1965, with links to pdf’s, at https://jansweb.netlify.app/publication/. There are many, many people I have to thank for my scientific education. Sixty years is a long time, and consequently many excellent teachers and researchers have crossed my path. I will gratefully mention the academics who had a major influence on my work and who are not with us any more, since I will join them in the not too distant future: Louis Guttman (died 1987), Clyde Coombs (died 1988), Warren Torgerson (died 1999), Forrest Young (died 2006), John van de Geer (died 2008), Joe Kruskal (died 2010), Doug Carroll (died 2011), and Rod McDonald (died 2012). References Borg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. Second Edition. Springer. Coombs, C. H. 1964. A Theory of Data. Wiley. De Leeuw, J. 1968. “Nonmetric Multidimensional Scaling.” Research Note 010-68. Department of Data Theory FSW/RUL. ———. 1973. “Canonical Analysis of Categorical Data.” PhD thesis, University of Leiden, The Netherlands. ———. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company. ———. 1984. Canonical Analysis of Categorical Data. Leiden, The Netherlands: DSWO Press. ———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80. De Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press. ———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company. ———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company. De Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03. Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38. Gifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley. Lingoes, J. C., and E. E. Roskam. 1973. “A Mathematical and Empirical Analysis of Two Multidimensional Scaling Algorithms.” Psychometrika 38: Monograph Supplement. Roskam, E. E. 1968. “Metric Analysis of Ordinal Data in Psychology.” PhD thesis, University of Leiden. Takane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67. Van de Geer, J. P. 1967. Inleiding in de Multivariate Analyse. Van Loghum Slaterus. ———. 1971. Introduction to Multivariate Analysis for the Social Sciences. San Francisco, CA: Freeman. "],["notation-and-reserved-symbols.html", "Notation and Reserved Symbols 0.1 Spaces 0.2 Matrices 0.3 Functions 0.4 MDS", " Notation and Reserved Symbols intro 0.1 Spaces \\(\\mathbb{R}^n\\) is the space of all real vectors, i.e. all \\(n\\)-element tuples of real numbers. Typical elements of \\(\\mathbb{R}^n\\) are \\(x,y,z\\). The element of \\(x\\) in position \\(i\\) is \\(x_i\\). Defining a vector by its elements is done with \\(x=\\{x_i\\}\\). \\(\\mathbb{R}^n\\) is equipped with the inner product \\(\\langle x,y\\rangle=x&#39;y=\\sum_{i=1}^nx_iy_i\\) and the norm \\(\\|x\\|=\\sqrt{x&#39;x}\\). The canonical basis for \\(\\mathbb{R}^n\\) is the \\(n-\\)tuple \\((e_1,cdots,e_n)\\), where \\(e_i\\) has element \\(i\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|e_i\\|=1\\) and \\(\\langle e_i,e_j\\rangle=\\delta^{ij}\\), with \\(\\delta^{ij}\\) the Kronecker delta (equal to one if \\(i=j\\) and zero otherwise). Note that \\(x_i=\\langle e_i,x\\rangle\\). \\(\\mathbb{R}\\) is the real line and \\(\\mathbb{R}_+\\) is the half line of non-negative numbers. \\(\\mathbb{R}^{n\\times m}\\) is the space of all \\(n\\times m\\) real matrices. Typical elements of \\(\\mathbb{R}^{n\\times m}\\) are \\(A,B,C\\). The element of \\(A\\) in row \\(i\\) and column \\(j\\) is \\(a_{ij}\\). Defining a matrix by its elements is done with \\(A=\\{a_{ij}\\}\\). \\(\\mathbb{R}^{n\\times m}\\) is equipped with the inner product \\(\\langle A,B\\rangle=\\text{tr} A&#39;B=\\sum_{i=1}^n\\sum_{j=1}^ma_{ij}b_{ij}\\) and the norm \\(\\|A\\|=\\sqrt{\\text{tr}\\ A&#39;A}\\). The canonical basis for \\(\\mathbb{R}^{n\\times m}\\) is the \\(nm-\\)tuple \\((E_{11},cdots,E_{nm})\\), where \\(E_{ij}\\) has element \\((i,j)\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|E_{ij}\\|=1\\) and \\(\\langle E_{ij},E_{kl}\\rangle=\\delta^{ik}\\delta^{jl}\\). \\(\\text{vec}\\) and \\(\\text{vec}^{-1}\\) 0.2 Matrices \\(a_{i\\bullet}\\) is row \\(i\\) of matrix \\(A\\), \\(a_{\\bullet j}\\) is column \\(j\\). \\(a_{i\\star}\\) is the sum of row \\(i\\) of matrix \\(A\\), \\(a_{\\star j}\\) is the sum of column \\(j\\). \\(A&#39;\\) is the transpose of \\(A\\), and \\(\\text{diag}(A)\\) is the diagonal matrix with the diagonal elements of \\(A\\). The inverse of a square matrix \\(A\\) is \\(A^{-1}\\), the Moore-Penrose generalized inverse of any matrix \\(A\\) is \\(A^+\\). If \\(A\\) and \\(B\\) are two \\(n\\times m\\) matrices then their Hadamard (or elementwise) product \\(C=A\\times B\\) has elements \\(c_{ij}=a_{ij}b_{ij}\\). The Hadamard quotient is \\(C=A/B\\), with elements \\(c_{ij}=a_{ij}/b_{ij}\\). The Hadamard power is \\(A^{(k)}=A^{(p-1)}\\times A\\). DC matrices. Centering matrix. \\(J_n=I_n-n^{-1}E_n\\). We do not use gthe subscripts if the order is obvious from the context. 0.3 Functions \\(f,g,h,\\cdots\\) are used for functions or mappings. \\(f:X\\rightarrow Y\\) says that \\(f\\) maps \\(X\\) into \\(Y\\). \\(\\sigma\\) is used for all real-valued least squares loss functions. 0.4 MDS \\(\\Delta=\\{\\delta_{ij\\cdots}\\}\\) is a matrix or array of dissimilarities. \\(\\langle \\mathbb{X},d\\rangle\\) is a metric space, with \\(d:\\mathcal{X}\\otimes\\mathcal{X}\\rightarrow\\mathbb{R}_+\\) the distance function. If \\(X\\) is is an ordered n-tuple \\((x_1,\\cdots,x_n)\\) of elements of \\(\\mathcal{X}\\) then \\(D(X)\\) is \\(\\{d(x_i,x_j)\\}\\), the elements of which we also write as \\(d_{ij}(X)\\). Summation over the elements of vector \\(x\\in\\mathbb{R}^n\\) is \\(\\sum_{i=1}^n x_i\\). Summation over the elements of matrix \\(A\\in\\mathbb{R}^{n\\times m}\\) is \\(\\sum_{i=1}^n\\sum_{j=1}^m a_{ij}\\). Summation over the elements above the diagonal of \\(A\\) is \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}a_{ij}\\). Conditional summation is, for example, \\(\\sum_{i=1}^n \\{x_i\\mid x_i&gt;0\\}\\). Iteration "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
