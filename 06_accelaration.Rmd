# Acceleration of Convergence {#chacceleration}

## Simple Acceleration {#accelsimple}

A simple and inexpensive way to accelerate smacof iterations was proposed by    
@deleeuw_heiser_C_80. 

On the other hand, if we choose $X=2\Gamma(Y)-Y$ then again $X\not= Y$, but
$\eta^2(X-\Gamma(Y))=\eta^2(Y-\Gamma(Y))$. Thus
\begin{equation}
\sigma(X)\leq 1+\eta^2(X-\Gamma(Y))-\eta^2(\Gamma(Y))=
1+\eta^2(Y-\Gamma(Y))-\eta^2(\Gamma(Y))=\sigma(Y).
(\#eq:upbmajupb)
\end{equation}
Let's define the two update rules $\text{up}_A(X):=\Gamma(X)$ and $\text{up}_B(X)=2\Gamma(X)-X$.

```{r updtstrategy, echo = FALSE}
library(polynom)
f<-polynomial(c(1,1,-.6,-.3,.01))
plot(f,xlim=c(-4,2), lwd = 4, col = "RED")
g <- deriv(f)
h <- min(solve(g))
fh <- predict(f, h)
gg <- deriv(g)
y <- -3
gp <- predict(g, y)
fp <- predict(f, y)
gy <- polynomial (c(fp - gp * y + 4 * y ^ 2, gp - 8 * y, 4))
fy <- as.function(gy)
curve(fy, from=-4, to = 2, lwd = 3, col = "BLUE", add = TRUE)
abline(v=-3, lwd = 2)
ymin <- solve(deriv(gy))
abline(h=fp, lwd = 2)
yext <- 2 * ymin - y
abline(v=yext, lwd = 2)
abline(v=ymin, lwd = 2)
```

This is illustrated in figure .... We want to locate a local minimum of $f$, in red, 
in the interval $(-4,2)$. In this case we happen to know that $f$ is a quartic polynomial, with minimum `r fh` at `r h`.  In the interval we are looking at we have $f''(x)\leq 8$. Suppose our initial guess for the location of the minimum is $x=-3$, the first vertical line from the left, with $f(-3)$ equal to `r fp`. The upper bound on the second derivative allows us to construct a quadratic majorizer $g$, in blue, touching $f$ at $-3$. Update rule $\text{up}_A$ tells us to go to the minimum of $g$, which is at `r ymin`, the second vertical line. Here $g$ is equal to
`r predict(gy, ymin)` and $f$ is `r predict(f, ymin)`.

Rule $\text{up}_B$ "overrelaxes" and goes all the way to `r yext`, the third vertical line from the left, where $g$ is equal to both $g(-3)$ and $f(-3)$, and where $f$ is `r predict(f, yext)`, indeed much closer to the minimum. Examples such as this make $\text{up}_B$ look good.

De Leeuw and Heiser give a rather informal theoretical justification of $\text{up}_B$ as well. Suppose the sequence $X^+=\Gamma(X)$ generated by $\text{up}_A$ has slow linear convergence with ACR $1-\epsilon$, where $\epsilon$ is positive and small. Then choosing the  $\text{up}_B$ will change the ACR of $1-\epsilon$ to  $2(1-\epsilon)-1=1-2\epsilon\approx(1-\epsilon)^2$, and will approximately halve the number of iterations to convergence. This argument is supported by numerical experiments which seem to show that indeed about half the number of iterations are needed. It seems that $\text{up}_B$ will get you something for almost nothing, and thus it has been implemented in various versions of the smacof programs as the default update. Unfortunately this may mean that many users have obtained, and presumably reported, MDS results that are incorrect.

What is ignored in @deleeuw_heiser_C_80 is that majorization only guarantees that the sequence of loss function values converges for both update methods. The general convergence theory discussed earlier in this chapter shows that for both $\text{up}_A$ and $\text{up}_B$ the sequence $\{X^{(k)}\}$ has at least one accumulation point, and that the accumulation points of the sequence $\{X^{(k)}\}$ are fixed points of the update rule, which means for both $\text{up}_A$ and $\text{up}_B$ that at accumulation points $X$ we have $X=\Gamma(X)$.  But it does **not** say that $\{X^{(k)}\}$ converges.

The argument also ignores that at any $X$ the derivative of $\text{up}_A$ has a zero eigenvalue, with eigenvector $X$. For $\text{up}_B$ the eigenvector $X$ has eigenvalue equal to $-1$, which is the largest one in modulus near any local minimum. And so ...

Suppose we have a configuration of the form $\alpha X$ with $X=\Gamma(X)$.
Then $\text{up}_B(\alpha X)=2\Gamma(\alpha X)-\alpha X=(2-\alpha)X$ and
$\text{up}_B((2-\alpha)X)=\alpha X$. Thus starting with $X^{(1)}=\alpha X$ 
$\text{up}_B$ generates a sequence with even members $(2-\alpha)X$ and odd members
$\alpha X$. Thus there are two convergent subsequences with accumulation points
$\alpha X$ and $(2-\alpha)X$. And never the twain shall meet.    

As far as stress is concerned, note that if $X=\Gamma(X)$ then $\sigma(\alpha X)=\sigma((2-\alpha)X)$. Thus the stress values never change, and consequently form a convergent sequence. 

We also see that $\text{up}^{(2)}_B(\alpha X):=\text{up}_B(\text{up}_B(\alpha X))=\alpha X$, which means that $\alpha X$ is a fixed point of $\text{up}_B^{(2)}$ for any fixed point $X$ of $\text{up}_A$ and any $\alpha$.

Another way to express the difference between the two update rule is that $\text{up}_A$
is *self-scaling*, i.e. $\Gamma(\alpha X)=\Gamma(X)$, while $\text{up}_B$
is not. Self-scaling implies $\mathcal{D}\Gamma(X)(X)=0$, while for $\text{up}_B$
$\mathcal{D}(2\Gamma(X)-X)(X)=-X$.
 
Let's now look at a real example. We use the Ekman color similarity data again, this time transformed by $\delta_{ij}=(1-s_{ij})^3$, The analysis is in two dimensions, with no weights.  We run four analyses, by crossing update rules $\text{up}_A$ and $\text{up}_B$ with stopping criteria  $\sigma(X^{(k)})-\sigma(X^{(k+1)})<\epsilon$ and $\max_{i,s}|x^{(k)}_{is}-x^{(k+1)}_{is}|<\epsilon$. Let's call these stopping criteria *stop_s* and *stop_x*. In all cases we allow a maximum of 1000 iterations and we set $\epsilon$ to 1e-10.

```{r ekstrategydata, echo = FALSE}
data(ekman, package = "smacof")
delta <- as.matrix ((1-ekman) ^ 3)
w <- 1 - diag(14)
m <- 91
delta <- delta / sqrt(m /sum (w * delta ^ 2))
v <- smacofVmatR(w)
```
```{r updsinglestep, echo = FALSE}
h10 <- smacofRelaxR(w, delta, 2, strategy = 1, xstop = FALSE)
h11 <- smacofRelaxR(w, delta, 2, strategy = 1, xstop = TRUE)
h20 <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE)
h21 <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE)
```

The results are in table ... The first subtable gives the number of iterations, the second the final stress value.
We see that generally stop_x requires more iterations than stop_s, because it is a stricter criterion. If we use
stop_x then $\text{up}_B$ does not converge at all. Both with stop_s and stop_x $\text{up}_B$ gves a higher stress value than $\text{up}_A$. And yes, with stop_s (which is the default stop criterion in the smacof programs so far)
$\text{up}_B$ use fewer iterations than $\text{up}_A$.

```{r echo = FALSE, cache = TRUE, results = "asis"}
ha <- data.frame("stop_f"= c(h10$itel, h20$itel), "stop_x" = c(h11$itel, h21$itel), 
                row.names=c("rule A", "rule B"))
hb <- data.frame("stop_f"= c(h10$s, h20$s), "stop_x" = c(h11$s, h21$s), 
                row.names=c("rule A", "rule B"))
kable(list(ha,hb))
```

```{r echo = FALSE, cache = TRUE}
mga <- formatC(max(abs(smacofGradientR(h11$x, h11$b, v))), width = 15, digits = 10, format = "f")
mgb <- formatC(max(abs(smacofGradientR(h21$x, h21$b, v))), width = 15, digits = 10, format = "f")
```

To verify that something is seriously wrong with running $\text{up}_B$, we compute the maximum absolute value of the gradient at convergence for both rules and stop_s. For $\text{up}_A$ it is 
`r mga` and for $\text{up}_B$ it is `r mgb`. 
Once again, with $\text{up}_B$ both loss function and configuration converge to an incorrect value. 

This can also be illustrated
graphically. We see from table ... that $\text{up}_B$ with stop_x ends after 1000 iteration. We perform an extra iteration, number 1001, and see how the configuration changes. In figure ... iteration 1000 is in black, iteration 1001 in red with slightly bigger characters. Except for a scaling factor the two configurations are the same.
Elementwise dividing the $\text{up}_B$ by the $\text{up}_A$ final configuration gives a shrinkage factor $\alpha$ of `r mean(h21$x/h11$x)`. This shrinkage factor can also be computed from the final stress values. Using $\rho(X)=\eta^2(X)$ and $\sigma(X)=1-\eta^2(X)$ we find $\sigma(\alpha X)-\sigma(X)=(\alpha-1)\eta^2(X)$, and thus
\begin{equation}
\alpha=1\pm\sqrt{\frac{\sigma(\alpha X)-\sigma(X)}{1-\sigma(X))}}.
(\#eq:minshrink)
\end{equation}

There are two values $\alpha$ and $2-\alpha$, equal to `r 1-sqrt((h21$s-h11$s) / (m - h11$s))` and
`r 1+sqrt((h21$s-h11$s) / (m - h11$s))`, because the sequence has two accumulation points.


```{r echo = FALSE}
par(pty="s")
x <- h21$x
d <- as.matrix(dist(x))
s <- smacofLossR (d, w, delta)
b <- smacofBmatR (d, w, delta)
vinv <- ginv (smacofVmatR (w))
z<- 2 * smacofGuttmanR (x, b, vinv) - x
plot(x, type = "n")
text(x, labels(delta)[[1]])
text(z, labels(delta)[[1]], col = "RED", cex = 1.5)
```

Things do not look good for $\text{up}_B$ but simple remedies are available. The first one is renormalization. After
the iterations, with say stop_s, have converged, we scale the configuration such that $\rho(X)=\eta^2(X)$ and recompute stress. This corrects both stress and the confguration to the correct outcome. Another way to normalize is to
do another single $\text{up}_A$ step after convergence of $\text{up}_B$. This has the same effect. We tried $\text{up}_B$
with both renormalization approaches and both stop_s and stop_b.  The number of $\text{up}_B$ iterations
is still the same as in table ... because we just compute something additional at the end. All stress values for the four combinations are now the correct `r h10$s`. It seems that using $\text{up}_B$ with stop_s and renormalization at the end gives us the best of both worlds. It accelerates convergence and it gives the correct loss function values.
                    
```{r echo = FALSE, cache = TRUE}
h20r <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE, renormalize = 1)
h21r <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE, renormalize = 1)
h20s <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = FALSE, renormalize = 2)
h21s <- smacofRelaxR(w, delta, 2, strategy = 2, xstop = TRUE, renormalize = 2)
```



```{r echo = FALSE, cache = TRUE}
h202 <- smacofRelaxR(w, delta, 2, strategy = 3, xstop = TRUE, renormalize = 1)
h212 <- smacofRelaxR(w, delta, 2, strategy = 4, xstop = TRUE, renormalize = 1)
```

Of course $\text{up}_B$ with stop_x still does not converge, and probably the best way to deal with that unfortunate fact is to avoid the combination alltogether. 

We can still use stop_x and get acceleration by define a single interation as $\text{up}_{AB}(x):=\text{up}_A(\text{up}_B(X))$. 
For comparison purposes we also run $\text{up}_{AA}(x):=\text{up}_A(\text{up}_A(X))$. Both converge to the correct values, $\text{up}_{AA}$ in `r h202$itel` and $\text{up}_{AB}(x)$ in `r h212$itel` iterations. 

Again $\text{up}_{AB}$ is an attractive strategy. It works with both
stop_s and stop_x and it accelerates. Less so than $\text{up}_B$, however. If the ACR of $\text{up}_A$ is $1-\epsilon$, then, by the same reasoning as before, the ACR of $\text{up}_{AB}$ is $(1-\epsilon)^\frac32$.

## One-Parameter Methods

In psychometrics, and perhaps in multivariate analysis, @ramsay_75 was the first to apply a general acceleration methods to sequences in $\mathbb{R}^n$ of the form $x^{(k+1)}=f(x^{(k)})$. 

@deleeuw_R_06b

## SQUAREM

## Vector Extrapolation Methods

@deleeuw_R_08h

@deleeuw_R_08i

@sidi_17


